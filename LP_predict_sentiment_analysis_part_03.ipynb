{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "LP_predict_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheBlackRus/Manning_LP_deploy_ml_as_microservice/blob/main/LP_predict_sentiment_analysis_part_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnBEijqExmWy"
      },
      "source": [
        "## Part 3: Deploying as a FaaS\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part3/predict_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A62fz1OLxmW3"
      },
      "source": [
        "### Download corpuses\n",
        "\n",
        "Since we won't be doing any model-training in this step, we don't need the 'movie_reviews' corpus. However, we will still need to extract features from our input before each prediction, so we make sure 'punkt' and 'stopwords' are available for tokenization and stopword-removal. If you added any other corpuses in Part 2, consider whether they'll be needed in the prediction step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RByTXLdPxmW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959be639-efc2-4865-a97f-b6673bfa615c"
      },
      "source": [
        "from nltk import download\n",
        "\n",
        "download('punkt')\n",
        "download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECuQCYjxmW4"
      },
      "source": [
        "### Define feature extractor and bag-of-words converter\n",
        "\n",
        "IMPORTANT: your predictions will only work properly if you use the same feature extractor that you trained your model with, so copy your updated `extract_features` method over from Part 2, replacing the method below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koy4DR03xmW4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "stopwords_eng = stopwords.words('english')\n",
        "#better_words = ['boring', 'stupid', 'marvelous', 'wonderful', 'ludicrous', 'sucks', 'apparently', 'filmmakers', 'astounding', 'avoids', 'atrocious', 'worst', 'magnificent', 'best', 'stark', 'strengths', 'outstanding', 'imaginative', 'strongest', 'headache', 'insulting', 'breathtaking', 'finest', 'illogical', 'dream', 'accessible', 'effective', 'regard', 'team', 'elliot', 'controversial', 'wasted', 'great', 'palpable', 'keen', 'unbearable', 'fascination', 'seamless', 'sans', 'conveys', 'hilarious', 'exquisite', 'country', 'questioning', 'embarrassing', 'matches', 'profit', 'literal', 'fairness', 'shannon', 'wonderfully', 'saddled', 'mcconaughey', 'sundance', 'secondly', 'memorable', 'idiotic', 'depicted', 'uninvolving', 'vulnerability', 'gaining', 'manipulation', 'shoddy', 'hatred', 'yawn', 'naval', 'ugh', 'treasure', 'frances', 'fishing', 'farther', 'anger', 'bad', 'chilling', 'best', 'insipid', 'uplifting', 'everyday', 'bore', 'commanding', 'quite', 'feeble', 'rude', 'detract', 'goo', 'aliens', 'nasa', 'mark', 'bad', 'mixing', '4', 'gump', 'surrealistic', 'gives', 'dread', 'liam', 'kurt', 'apocalyptic', 'poetry', 'glasses']\n",
        "def extract_features(words):\n",
        "    return [w for w in words if w not in stopwords_eng and w not in punctuation ]#and (w in better_words)]\n",
        "\n",
        "def bag_of_words(words):\n",
        "    bag = {}\n",
        "    for w in words:\n",
        "        bag[w] = bag.get(w,0)+1\n",
        "    return bag"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ay3lC28exmW4"
      },
      "source": [
        "### Import your pickled model file (non-Colab version)\n",
        "\n",
        "In Part 2, we saved the trained model as \"sa_classifier.pickle\". Now we'll unpickle that file to get it back into memory. Either copy that file into the same folder as this notebook (\"part3\"), or adjust the path below to \"../part2/sa_classifier.pickle\" so it reads the file from the folder where it was saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPARyQ9ryYC2"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHEiPpt1yb7K",
        "outputId": "16a9b348-085e-48b7-cbe4-21b159bd3e05"
      },
      "source": [
        "os.listdir(\".\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'sa_classifier_better.pickle',\n",
              " 'sa_classifier.pickle',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT5VYcpAxmW5"
      },
      "source": [
        "import pickle\n",
        "import sys\n",
        "\n",
        "#if not 'google.colab' in sys.modules:\n",
        "#model_file = open('./sa_classifier_better.pickle', 'rb')\n",
        "model_file = open('./sa_classifier.pickle', 'rb')\n",
        "model = pickle.load(model_file)\n",
        "model_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH4VYGs9xmW5"
      },
      "source": [
        "### Import your pickled model file (Colab version)\n",
        "\n",
        "If you're running this notebook on Colab, we need to retrieve the pickled model from [Google Drive](https://drive.google.com) before we can unpickle it. This code looks for \"sa_classifier.pickle\" in a folder called \"Colab Output\"; if you have moved the file elsewhere, change the path below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8DRL28JwxmW5"
      },
      "source": [
        "import pickle\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    !ls '/content/gdrive/My Drive/Colab Output'\n",
        "    model_file = open('/content/gdrive/My Drive/Colab Output/sa_classifier.pickle','rb')\n",
        "    model = pickle.load(model_file)\n",
        "    model_file.close()\n",
        "    print('Model loaded from /content/gdrive/My Drive/Colab Output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiKiLJWFxmW6"
      },
      "source": [
        "### Define a method for prediction\n",
        "\n",
        "In the prediction step, we'll be taking a single piece of text input and asking the model to classify it. Models need the input for the prediction step to have the same format as the data provided during training -- so we must tokenize the input, run the same `extract_features` method that we used during training, and convert it to a bag of words before sending it to the model's `classify` method.\n",
        "\n",
        "Note: if you have (from Part 2) changed your `extract_features` method to accept the full text instead of a tokenized list, then you can omit the tokenization step here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVP-ankPxmW6"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def get_sentiment(review):\n",
        "    words = word_tokenize(review)\n",
        "    words = extract_features(words)\n",
        "    words = bag_of_words(words)\n",
        "    return model.classify(words)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YavXuTdzxmW6"
      },
      "source": [
        "### Run a prediction\n",
        "\n",
        "Test out your `get_sentiment` method on some sample inputs of your own devising: try altering the two reviews below and see how your model performs. It won't be 100% correct, and we're mostly just looking to see that it is able to run at all, but if it sems to *always* be wrong, that may indicate you've missed a critical step above (e.g. you haven't copied over all the changes to your feature extractor from Part 2, or you've loaded the wrong model file, or provided un-tokenized text when a list of words was expected)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWib4nLmxmW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43f8c8f-6369-4196-b9f6-50cdf8e70027"
      },
      "source": [
        "positive_review = 'This movie is amazing, with witty dialog and beautiful shots.'\n",
        "print('positive_review: '+get_sentiment(positive_review))\n",
        "\n",
        "negative_review = 'I hated everything about this unimaginitive mess. Two thumbs down!'\n",
        "print('negative_review: '+get_sentiment(negative_review))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive_review: pos\n",
            "negative_review: neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "viCe79sGMxgL",
        "outputId": "1684b4b2-b954-49a2-e67b-cf3e579aa3e5"
      },
      "source": [
        "#%pip install Algorithmia"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Algorithmia\n",
            "  Downloading https://files.pythonhosted.org/packages/77/e0/5e8d8794e0f07ac993524e2bce3c4168c73269b0ec2d030118cb46d1e9e6/algorithmia-1.8.0-py2.py3-none-any.whl\n",
            "Collecting enum-compat\n",
            "  Downloading https://files.pythonhosted.org/packages/55/ae/467bc4509246283bb59746e21a1a2f5a8aecbef56b1fa6eaca78cd438c8b/enum_compat-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Algorithmia) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Algorithmia) (2.23.0)\n",
            "Collecting algorithmia-adk<1.1,>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/7a/bde356f95cb4c10b92b7af4ffa8a137876bdff8f437b9b827ad318310aed/algorithmia_adk-1.0.2-py2.py3-none-any.whl\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Collecting algorithmia-api-client==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/1b/9e8d578c72863b8bef58ec4d62d6fd654417e476498fa39b67c77be65c8a/algorithmia_api_client-1.3.1-py2.py3-none-any.whl (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from Algorithmia) (0.10.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Algorithmia) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Algorithmia) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Algorithmia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Algorithmia) (2.10)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from algorithmia-api-client==1.3.1->Algorithmia) (2.8.1)\n",
            "Installing collected packages: enum-compat, algorithmia-adk, argparse, algorithmia-api-client, Algorithmia\n",
            "Successfully installed Algorithmia-1.8.0 algorithmia-adk-1.0.2 algorithmia-api-client-1.3.1 argparse-1.4.0 enum-compat-0.0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "F-Yiln2dxmW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1f0a79-8255-45e5-ae18-909d771269db"
      },
      "source": [
        "import Algorithmia\n",
        "\n",
        "input = \"This is a great movie!\"\n",
        "client = Algorithmia.client(API_KEY)\n",
        "algo = client.algo('The_Black_Rus/lp_alg/1.0.0')\n",
        "algo.set_options(timeout=300) # optional\n",
        "print(algo.pipe(input).result)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou9mzRKYMvFV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}